{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The Bag of Words (BoW) model is a popular text representation technique used in natural language processing (NLP) and machine learning. It’s designed to convert text into numerical data by focusing on the presence or absence of words, ignoring grammar and word order."
      ],
      "metadata": {
        "id": "iLMRl_cXDgHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "oXrTUZ1vKRPL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"today i am feeling great.i was so sick for past days. today is a sunday. now sickness is gone.\"]\n",
        "# Each item in this list represents a document, and these documents will be processed to count each unique word’s frequency."
      ],
      "metadata": {
        "id": "3cFfxaGmKTQ9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoW focuses solely on word frequency, not meaning derived from syntax or sentence structure."
      ],
      "metadata": {
        "id": "07ieLNtJKiz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()    # here, each word is assigned a count (frequency) based on its occurrence in the document."
      ],
      "metadata": {
        "id": "48Wa62T7KjXz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer, which will handle:\n",
        "\n",
        "- Tokenization (splitting text into individual words),\n",
        "- Lowercasing (making all words lowercase),\n",
        "- Filtering (removing punctuation or any unnecessary symbols)."
      ],
      "metadata": {
        "id": "l_NqG9h7ObWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix = vectorizer.fit_transform(texts)"
      ],
      "metadata": {
        "id": "XU7OkoQQLu76"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit_transform() does two things:\n",
        "\n",
        "- Fit: Learns the vocabulary of all unique words across the texts.\n",
        "- Transform: Converts each document into a numerical vector representing the frequency of each word."
      ],
      "metadata": {
        "id": "ELuN9VC9Ontk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The BoW Matrix is:\", bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Iq8klmLu4W",
        "outputId": "d128d958-89e7-41a5-ee28-09c35369291c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BoW Matrix is: [[1 1 1 1 1 1 2 1 1 1 1 1 1 2 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Bag of Words (BoW) model in this code creates a matrix showing word frequencies for each document based on a fixed vocabulary.\n",
        "- It’s commonly used for text classification and sentiment analysis but doesn’t consider the order of words, focusing purely on word occurrence frequency across the text."
      ],
      "metadata": {
        "id": "L1f5K02lQC0m"
      }
    }
  ]
}