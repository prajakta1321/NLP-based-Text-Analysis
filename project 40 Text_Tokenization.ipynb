{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text tokenization is the process of reformatting a piece of text into smaller units called “tokens.”\n",
        "\n",
        "It transforms unstructured text into structured data that models can understand.\n",
        "\n",
        "The goal of tokenization is to break down text into meaningful units like words, phrases, sentences, etc. which can then be inputted into machine learning models.\n",
        "\n",
        "Tokenization enables natural language processing tasks like part-of-speech tagging (identifying verbs vs nouns, etc.), named entity recognition (categories like person, organization, location), and relationship extraction (family relationships, professional relationships, etc.)."
      ],
      "metadata": {
        "id": "F7lVOFxjiN3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "x2-msLRKiSrY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGSYyGbKiX8p",
        "outputId": "2c344006-c257-4a7b-a6df-3e49987903fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Today is a friday and its so sunny outside!\""
      ],
      "metadata": {
        "id": "YKIg1K7-iehV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing the sentence above\n",
        "\n",
        "tokenized = nltk.tokenize.word_tokenize(text)"
      ],
      "metadata": {
        "id": "tjnP0TgAiqZS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgFhvhq2i_Pv",
        "outputId": "bd30ebd7-b05f-40d3-91ec-ffd02980e2e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Today', 'is', 'a', 'friday', 'and', 'its', 'so', 'sunny', 'outside', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "qK0OMF0YiwcI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "XCT3nENxi1q-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized = vectorizer.fit_transform(tokenized).toarray()"
      ],
      "metadata": {
        "id": "-KMkIJTIi4_B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NBciVDTi87L",
        "outputId": "a6f89857-ed02-4aee-dd4e-5d0f4bc17cf7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using sklearn\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "OQDsOKtwjhQl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "YS7pa4eFjkF7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_matrix = vectorizer.fit_transform(tokenized)"
      ],
      "metadata": {
        "id": "Aoej2ctujmx1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sparse_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwse2hOjjtbh",
        "outputId": "6a7fb16a-fe60-4e07-c521-a1bda0aabf7d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 7)\t1.0\n",
            "  (1, 2)\t1.0\n",
            "  (3, 1)\t1.0\n",
            "  (4, 0)\t1.0\n",
            "  (5, 3)\t1.0\n",
            "  (6, 5)\t1.0\n",
            "  (7, 6)\t1.0\n",
            "  (8, 4)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer and TfidfVectorizer are both classes in scikit-learn's feature extraction module, but they serve different purposes in the context of text data preprocessing.\n",
        "\n",
        "CountVectorizer:\n",
        "\n",
        "It counts the occurrences of each word in the document.\n",
        "The output is a sparse matrix where each row corresponds to a document, and each column corresponds to a unique word in the entire corpus.\n",
        "The matrix elements contain the count of each word in the respective document.\n",
        "\n",
        "TfidfVectorizer:\n",
        "\n",
        "It computes the Term Frequency-Inverse Document Frequency (TF-IDF) values for each word in the document.\n",
        "TF-IDF takes into account the frequency of a word in a document relative to its frequency across all documents in the corpus.\n",
        "The output is a sparse matrix where each row corresponds to a document, and each column corresponds to a unique word in the entire corpus.\n",
        "The matrix elements contain the TF-IDF values of each word in the respective document."
      ],
      "metadata": {
        "id": "HnJ4rqJXj8u3"
      }
    }
  ]
}