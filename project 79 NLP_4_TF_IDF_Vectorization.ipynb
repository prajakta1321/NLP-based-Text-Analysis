{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###TF IDF###\n",
        "\n",
        "- TF-IDF stands for Term Frequency-Inverse Document Frequency, a statistical measure used in Natural Language Processing (NLP) to evaluate the importance of a word in a document relative to a collection of documents (corpus).\n",
        "\n",
        "- TF-IDF is a widely used technique to weigh the importance of words in documents while down-weighting common words that appear in many documents, making it extremely useful for many NLP tasks.\n",
        "\n",
        "\n",
        "\n",
        "##Why Use TF-IDF?##\n",
        "\n",
        "#Feature Extraction:\n",
        "\n",
        "TF-IDF is used to extract features from text for machine learning models. It converts textual data into numerical data that models can work with.\n",
        "\n",
        "#Information Retrieval:\n",
        "\n",
        "It helps in searching and ranking documents based on the relevance of the query. Words that appear more frequently in fewer documents are considered more important for ranking.\n",
        "\n",
        "#Text Classification:\n",
        "\n",
        "In classification tasks like spam detection or sentiment analysis, TF-IDF helps models understand which words are relevant to each category."
      ],
      "metadata": {
        "id": "i2IfVzd5J42x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  # it is used to convert a collection of raw text documents into a matrix of TF-IDF features."
      ],
      "metadata": {
        "id": "bsXaaeKbJ5It"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"Prajakta loved doing NLP. she also loved doing NLP with ML\"]"
      ],
      "metadata": {
        "id": "6pFvten2KE0u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Document 1: \"Prajakta loved doing NLP.\"\n",
        "\n",
        "- Document 2: \"She also loved doing NLP with ML.\""
      ],
      "metadata": {
        "id": "oPp2UO9wXU3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()   # This creates an instance of the TfidfVectorizer. It automatically handles tokenization (splitting words) and removes common English stopwords."
      ],
      "metadata": {
        "id": "2TgvWVLiKRo2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix = vectorizer.fit_transform(texts)"
      ],
      "metadata": {
        "id": "l8Z-399NKVVp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Fitting:\n",
        "\n",
        "The vectorizer.fit_transform(texts) learns the vocabulary of the input texts and computes the TF-IDF scores for each word.\n",
        "\n",
        "- Transforming:\n",
        "\n",
        "It then transforms each document into a sparse matrix of TF-IDF scores. The output tfidf_matrix contains the TF-IDF values for each word in each document."
      ],
      "metadata": {
        "id": "FeW5-PFHVww2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TF-IDF Matrix:\", tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEmnCO-aKVI5",
        "outputId": "f32f96cd-2dfe-4aa8-96e8-a16e49410e0b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix: [[0.24253563 0.48507125 0.48507125 0.24253563 0.48507125 0.24253563\n",
            "  0.24253563 0.24253563]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MHjXaYsXN3y",
        "outputId": "ad7e64b4-f1fb-4163-e7de-a0b410d0d0bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['also' 'doing' 'loved' 'ml' 'nlp' 'prajakta' 'she' 'with']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INTERPRETATION :\n",
        "\n",
        "- Prajakta and ML are unique to their respective sentences, so they get higher TF-IDF scores.\n",
        "\n",
        "- NLP and loved appear in both sentences, so their TF-IDF scores are lower due to lower inverse document frequency.\n"
      ],
      "metadata": {
        "id": "V4mZHl2bXjiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This shows how TF-IDF assigns importance to words based on their frequency in each sentence and the entire corpus.\n",
        "\n",
        "- Words that are common in both sentences (like \"NLP\" and \"loved\") have lower TF-IDF scores, while unique words (like \"Prajakta\" and \"ML\") have higher scores."
      ],
      "metadata": {
        "id": "loyz7hmqXsQj"
      }
    }
  ]
}